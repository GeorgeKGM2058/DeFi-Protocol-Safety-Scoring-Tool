{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "DeFi Protocol Safety Scoring Tool"
      ],
      "metadata": {
        "id": "TeuYdPLGSkUT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A lightweight Python-based risk assessment framework for decentralized finance, as of February 2026. Designed to run seamlessly in Google Colab, no heavy tools like Slither/Mythril\n",
        "\n",
        "\n",
        "\n",
        "1. Introduction\n",
        "\n",
        "The DeFi space is exciting but tricky—it's full of opportunities, but also packed with risks like wild price swings, liquidity drops and past hacks that still haunt some projects. This tool is my take on making sense of that chaos for a few big players: Aave, Uniswap and Compound.\n",
        "\n",
        "Basically, it pulls together market stats and historical security info to give each protocol a quick safety score. The financial side looks at things like TVL scale and recent trends, while the security side digs into known exploits and how fresh their audits are. The end result is a clean PDF report with scores, charts and notes on where the data came from.\n",
        "\n",
        "I kept it simple on purpose with no fancy blockchain security analysis tools like Slither, Mythril or endless dependencies, so anyone can run it in Colab seamlessly. It's not meant to be investment advice, just a practical way to spot relative risks. Think of it as a starting point for deeper dives."
      ],
      "metadata": {
        "id": "Z9RhfRm5mTT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 1 – Setup & Constants"
      ],
      "metadata": {
        "id": "-JkLVI7agsJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn fpdf2 beautifulsoup4 --quiet\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import requests\n",
        "import re\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from fpdf import FPDF\n",
        "from fpdf.enums import XPos, YPos\n",
        "from io import BytesIO\n",
        "from google.colab import files\n",
        "from datetime import datetime\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "print(\"Setup complete.\")\n",
        "\n",
        "PROTOCOLS = {\n",
        "    \"aave\": {\n",
        "        \"coingecko_id\": \"aave\",\n",
        "        \"defillama_slug\": \"aave\",\n",
        "        \"github_repo\": \"aave/aave-v3-core\",\n",
        "        \"immunefi_url\": \"https://immunefi.com/bug-bounty/aave/information\",\n",
        "        \"type\": \"lending\"\n",
        "    },\n",
        "    \"uniswap\": {\n",
        "        \"coingecko_id\": \"uniswap\",\n",
        "        \"defillama_slug\": \"uniswap\",\n",
        "        \"github_repo\": \"Uniswap/v4-core\",\n",
        "        \"immunefi_url\": \"https://cantina.xyz/bounties/f9df94db-c7b1-434b-bb06-d1360abdd1be\",\n",
        "        \"type\": \"dex\"\n",
        "    },\n",
        "    \"compound\": {\n",
        "        \"coingecko_id\": \"compound-governance-token\",\n",
        "        \"defillama_slug\": \"compound-finance\",\n",
        "        \"github_repo\": \"compound-finance/compound-protocol\",\n",
        "        \"immunefi_url\": \"https://immunefi.com/bug-bounty/compoundfinance/information\",\n",
        "        \"type\": \"lending\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(f\"Configured {len(PROTOCOLS)} protocols: {', '.join(PROTOCOLS.keys())}\")"
      ],
      "metadata": {
        "id": "l21OhV0S57J7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Methodology\n",
        "\n",
        "To build this, I focused on reliable, free data sources and straightforward calculations. No overcomplicating things with blockchain scanners or machine learning, just using APIs and basic math to keep it fast and explainable.\n",
        "\n",
        "\n",
        "Data Collection (APIs & Public Sources)\n",
        "\n",
        "Everything starts with grabbing fresh (or cached) data:\n",
        "Market metrics: From CoinGecko's free API. This gives price changes over 7 and 30 days, plus a simple volatility measure (absolute 30-day % shift). It's quick and covers token basics without needing keys.\n",
        "\n",
        "TVL and trends: Pulled from DefiLlama's protocol endpoint. Gets the current total locked value across chains and the last 30 daily snapshots for spotting declines. If the API hiccups (which happens in Colab), it falls back to recent cached values from early 2026 and notes that in the report.\n",
        "\n",
        "For security, it's all from public reports, with no live scanning. I hardcoded summaries based on checks from sites like Rekt.news for exploits, protocol docs for audits and Immunefi for bounties. Easy to update if needed.\n",
        "\n",
        "One thing to note: APIs can be flaky, so the code will flag when it uses backups. That way, you know if scores are live or not."
      ],
      "metadata": {
        "id": "d1yvYzwkm4ld"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 2 – Data Fetching Functions"
      ],
      "metadata": {
        "id": "JP0PRo43goef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_market_data(protocol):\n",
        "    config = PROTOCOLS[protocol]\n",
        "    url = f\"https://api.coingecko.com/api/v3/coins/{config['coingecko_id']}\"\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}                       # This reduces API blocks\n",
        "    try:\n",
        "        response_data = requests.get(url, headers=headers).json() # API request and parse JSON\n",
        "        if 'market_data' not in response_data:\n",
        "            raise KeyError('market_data')                         # to ensure the expected key exists\n",
        "        market_data_response = response_data[\"market_data\"]\n",
        "        calculated_volatility = abs(market_data_response.get(\"price_change_percentage_30d_in_currency\", {}).get(\"usd\", 0)) / 100\n",
        "        data = {\n",
        "            \"price_change_7d\": market_data_response[\"price_change_percentage_7d_in_currency\"][\"usd\"],\n",
        "            \"price_change_30d\": market_data_response[\"price_change_percentage_30d_in_currency\"][\"usd\"],\n",
        "            \"volatility\": calculated_volatility\n",
        "        }\n",
        "        data['source'] = 'live'\n",
        "        return data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Market data fetch failed for {protocol}: {e}\")\n",
        "        # 2026 updated fallbacks\n",
        "        if protocol == \"aave\":\n",
        "            data = {\"price_change_7d\": -15.0, \"price_change_30d\": 30.2, \"volatility\": 0.302}\n",
        "        elif protocol == \"uniswap\":\n",
        "            data = {\"price_change_7d\": -11.3, \"price_change_30d\": 35.2, \"volatility\": 0.352}\n",
        "        else:\n",
        "            data = {\"price_change_7d\": -20.6, \"price_change_30d\": 35.6, \"volatility\": 0.356}\n",
        "        data['source'] = 'fallback'\n",
        "        return data\n",
        "\n",
        "\n",
        "def fetch_tvl_data(protocol):\n",
        "    config = PROTOCOLS[protocol]\n",
        "    url = f\"https://api.llama.fi/protocol/{config['defillama_slug']}\"\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    try:\n",
        "        defillama_response = requests.get(url, headers=headers).json()\n",
        "        raw_tvl_history_data = defillama_response.get(\"tvl\", [])\n",
        "        tvl_history = [e[\"totalLiquidityUSD\"] for e in raw_tvl_history_data][-30:]     # most recent 30 days\n",
        "        chain_tvls = defillama_response.get(\"currentChainTvls\", {})\n",
        "        total_tvl = sum(v for k, v in chain_tvls.items() if '-borrowed' not in k.lower() and '-debt' not in k.lower())            # To handle potential key changes\n",
        "        borrowed = sum(v for k, v in chain_tvls.items() if '-borrowed' in k.lower() or '-debt' in k.lower())\n",
        "        chain_count = len(set(k.split('-')[0] for k in chain_tvls if '-borrowed' not in k.lower() and '-debt' not in k.lower()))  # Count unique chains supporting non-borrowed TVL\n",
        "\n",
        "        # Attempt to get 24h volume (the API doesn't have it but adding it for completion's sake)\n",
        "        volume_24h = defillama_response.get(\"volume_24h\")\n",
        "        if volume_24h is None:\n",
        "            if protocol == \"uniswap\":\n",
        "                volume_24h = 1.37e9\n",
        "            else:\n",
        "                volume_24h = 0        # Doesn't apply to lending\n",
        "\n",
        "        data = {\n",
        "            \"total_tvl\": total_tvl,\n",
        "            \"tvl_history\": tvl_history or [0] * 30,\n",
        "            \"borrowed\": borrowed,\n",
        "            \"chain_count\": chain_count,\n",
        "            \"volume_24h\": volume_24h\n",
        "        }\n",
        "        data['source'] = 'live'\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"TVL fetch failed for {protocol}: {e}\")\n",
        "        # DefiLlama data\n",
        "        if protocol == \"aave\":\n",
        "            data = {\"total_tvl\": 28.0e9, \"tvl_history\": [10e9] * 30, \"borrowed\": 7e9, \"chain_count\": 10, \"volume_24h\": 0}\n",
        "        elif protocol == \"uniswap\":\n",
        "            data = {\"total_tvl\": 3.105e9, \"tvl_history\": [5e9] * 30, \"borrowed\": 0, \"chain_count\": 40, \"volume_24h\": 1.37e9}\n",
        "        else:\n",
        "            data = {\"total_tvl\": 1.485e9, \"tvl_history\": [2e9] * 30, \"borrowed\": 0.6e9, \"chain_count\": 9, \"volume_24h\": 0}\n",
        "        data['source'] = 'fallback'\n",
        "        return data\n",
        "\n",
        "\n",
        "def fetch_utilization_and_chains(protocol, tvl_data):\n",
        "    config = PROTOCOLS[protocol]\n",
        "    try:\n",
        "        total_tvl = tvl_data['total_tvl']\n",
        "        borrowed = tvl_data['borrowed']\n",
        "        chains = tvl_data['chain_count']\n",
        "        source = tvl_data['source']\n",
        "\n",
        "        if config['type'] == 'lending':\n",
        "            util = (borrowed / (total_tvl + borrowed)) * 100 if (total_tvl + borrowed) > 0 else 0\n",
        "            metric_name = \"Utilization (Lending)\"\n",
        "        elif config['type'] == 'dex':\n",
        "            volume_24h = tvl_data['volume_24h']\n",
        "            util = (volume_24h / total_tvl) * 100 if total_tvl > 0 else 0  # Volume/TVL as efficiency proxy (e.g., 44% for Uniswap based on fallbacks)\n",
        "            util = min(util, 100)                                          # Cap to avoid outliers\n",
        "            metric_name = \"Efficiency (DEX - Vol/TVL)\"\n",
        "        else:\n",
        "            util = 0\n",
        "            metric_name = \"N/A\"\n",
        "    except:\n",
        "        print(f\"Util/chains calc failed for {protocol}\")\n",
        "        # Fallbacks\n",
        "        if protocol == \"aave\":\n",
        "            util = 20.0  # Based on fallback borrowed/TVL\n",
        "        elif protocol == \"uniswap\":\n",
        "            util = 44.1  # ~1.37B volume / 3.1B TVL\n",
        "        else:\n",
        "            util = 28.8  # Based on fallback\n",
        "        chains = PROTOCOLS[protocol].get('chain_count_fallback', 10)\n",
        "        source = 'fallback'\n",
        "\n",
        "    return {'utilization': util, 'chain_count': chains, 'source': source, 'metric_name': metric_name}"
      ],
      "metadata": {
        "id": "gJPaW-TNDkIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Risk Analysis (Financial & Historical Security)\n",
        "\n",
        "The core is two risk angles, blended into one score.\n",
        "\n",
        "Financial risk weighs current market health:\n",
        "- Bigger TVL means more stability (min(total_tvl / 1e9, 10), capped at 10 to avoid over-weighting giants like Aave).\n",
        "- Price momentum averages recent changes, positive if things are up, negative if down\n",
        "(average of 7d + 30d % change / 5 – divided by 5 to normalize large swings).\n",
        "- Volatility penalty: 30d absolute change × 2 – multiplier balances bear markets without overpowering. TVL trend uses a simple linear fit on the last 30 days, the downward slope pulls it lower.\n",
        "- Trend penalty, negative slope from linear regression / 5e8 – divisor tunes impact for typical TVL fluctuations.\n",
        "\n",
        "\n",
        "For historical security, it's about track record:\n",
        "- Starts high at 10, then subtracts for any exploits (3x heavier if recent).\n",
        "- More penalties for old or few audits, or noted vulns (even fixed ones, ×1.5 for multiples).\n",
        "- Bonuses for high audit count (/4 min 3) recency (2025+ preferred) andactive bug bountiy (+1).\n",
        "\n",
        "It's not perfect, it relies on public info, so it misses unreported issues. But it's transparent and based on verifiable events.\n",
        "\n",
        "\n",
        "\n",
        "Data Processing & Visualization\n",
        "\n",
        "Once data's in, processing is basic: linear regression for trends (sklearn), heatmaps/tables/charts with seaborn/matplotlib. Then fpdf assembles it into a PDF—summaries first, visuals after.\n",
        "I added source tracking (live vs fallback) to keep things honest and clear."
      ],
      "metadata": {
        "id": "c_uQIt-RnhG2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 3 – Analysis Functions (security history, trend prediction, scoring)"
      ],
      "metadata": {
        "id": "mAdES2v8gndA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_bounty_and_code_age(protocol):\n",
        "    config = PROTOCOLS[protocol]\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    try:\n",
        "        # Live code age from GitHub API: Fetches the date of the last commit in project activity.\n",
        "        github_url = f\"https://api.github.com/repos/{config['github_repo']}/commits?per_page=1\"     # GitHub API to get last commit\n",
        "        commit_data = requests.get(github_url, headers=headers).json()\n",
        "        last_commit_date = commit_data[0]['commit']['author']['date'].split('T')[0]\n",
        "        code_age_days = (datetime.now() - datetime.strptime(last_commit_date, \"%Y-%m-%d\")).days\n",
        "\n",
        "        # Live bounty from Immunefi/Cantina page using BS4 for parsing dynamic page structures to extract the maximum bounty size to reflect active security investment.\n",
        "        response = requests.get(config['immunefi_url'], headers=headers)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        for tag in soup.find_all([\"script\", \"style\"]):\n",
        "            tag.extract()          # Remove all irrelevant tags for cleaner text extraction\n",
        "\n",
        "        # Iterative search for common headings, to find the rewards section heading that indicates bounty information, usually near specific headers.\n",
        "        reward_heading = None\n",
        "        for heading in soup.find_all(['h1', 'h2', 'h3', 'h4']):\n",
        "            head_text = heading.text.lower()\n",
        "            if \"reward\" in head_text and any(word in head_text for word in [\"threat\", \"severity\", \"impact\", \"level\", \"tier\"]):\n",
        "                reward_heading = heading\n",
        "                break\n",
        "\n",
        "        if reward_heading:\n",
        "            section_content = []\n",
        "            current = reward_heading.next_element\n",
        "            # It collects content until the next major heading is found, to isolate the bounty description.\n",
        "            while current and not (hasattr(current, 'name') and current.name in ['h1', 'h2', 'h3', 'h4']):\n",
        "                if isinstance(current, str) and current.strip():\n",
        "                    section_content.append(current.strip())\n",
        "                current = current.next_element\n",
        "            section_text = ' '.join(section_content)    # Combines relevant text for bounty parsing\n",
        "        else:\n",
        "            section_text = soup.get_text(separator=' ')\n",
        "\n",
        "        slice_text = section_text.lower()[:4000]\n",
        "\n",
        "        keywords = [\"maximum reward\", \"max:\", \"critical\", \"rewards by threat level\", \"smart contract critical\"]       # Locating the approximate position of bounty figures using a set of keywords.\n",
        "        pos = min((slice_text.find(k) for k in keywords if slice_text.find(k) > -1), default=-1)                      # finds first occurrence of any bounty keyword\n",
        "        if pos > -1:\n",
        "            slice_text = slice_text[pos:pos + 2000]\n",
        "\n",
        "        # Regex designed to be flexible on variations like 'max:', 'critical', dollar signs, commas, spaces and decimal values, common in bounty listings.\n",
        "        matches = re.findall(r'(?:max(?:imum)?|critical)\\s*[:\\-\\+]?\\s*[\\$]?\\s*(\\d{1,3}(?:[,\\s]\\d{3})*(?:\\.\\d+)?)', slice_text, re.I)\n",
        "        bounty_size = 0\n",
        "        for m in matches:\n",
        "            try:\n",
        "                cleaned = m.replace(',', '').replace(' ', '')\n",
        "                num = float(cleaned)\n",
        "                if 'million' in slice_text.lower() and num < 1000000:\n",
        "                    num *= 1000000\n",
        "                if num > bounty_size and 500000 <= num <= 20000000:\n",
        "                    bounty_size = int(num)\n",
        "            except:\n",
        "                pass\n",
        "        if bounty_size < 900000:\n",
        "            raise ValueError(\"Parsed bounty suspiciously low or zero\")       # Check parsed bounty value logically\n",
        "        return {'bounty_size': bounty_size, 'code_age_days': code_age_days, 'source': 'live'}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Bounty/code fetch failed for {protocol}: {e}\")\n",
        "        # If the live fetching fails uses fallback values to ensure the report can still be generated.\n",
        "        if protocol == \"aave\":\n",
        "            bounty_size = 1000000\n",
        "            code_age_days = 30\n",
        "        elif protocol == \"uniswap\":\n",
        "            bounty_size = 15500000\n",
        "            code_age_days = 15\n",
        "        else:\n",
        "            bounty_size = 1000000\n",
        "            code_age_days = 1335\n",
        "        return {'bounty_size': bounty_size, 'code_age_days': code_age_days, 'source': 'fallback'}\n",
        "\n",
        "\n",
        "def check_security_history(protocol, bounty_code_data):\n",
        "    # 2026 data, simple and hardcoded to avoid overcomplication. Avoids complex or unreliable APIs for historical security events and their importance estimation.\n",
        "    history = {\n",
        "        \"aave\": {\n",
        "            \"exploit_count\": 0,       # No major core (tools: periphery minor 2025 $51K)\n",
        "            \"recent_vuln\": 2,         # 2023 mitigated, 2025 periphery (no loss)\n",
        "            \"audit_count\": 12,        # V3.6 2025 (5), V4 2026 (Sherlock no findings + final)\n",
        "            \"latest_audit_year\": 2026,\n",
        "            \"issues\": [\"2023 vulnerability mitigated, no loss; 2025 periphery hack $51K not core\"],\n",
        "            \"has_audits\": True,\n",
        "            \"audit_note\": \"V3.6 2025 (5 reports); V4 2026 (Sherlock contest no findings, final audits)\",\n",
        "            \"bounty_active\": True     # Active per tools\n",
        "        },\n",
        "        \"uniswap\": {\n",
        "            \"exploit_count\": 0,       # No core (tools: hook-related like Cork 2025 $11M but was not core)\n",
        "            \"recent_vuln\": 2,         # Hook vulns noted in audits, SIR 2025 exploit using V3 pools\n",
        "            \"audit_count\": 10,        # V4 2024 (9 audits + $2.35M contest no severe)\n",
        "            \"latest_audit_year\": 2024,\n",
        "            \"issues\": [\"No core exploits; hook vulnerabilities noted in audits; SIR 2025 using V3\"],\n",
        "            \"has_audits\": True,\n",
        "            \"audit_note\": \"V4 2024 (5+ in contest); multiple firms, $15.5M bug bounty active\",\n",
        "            \"bounty_active\": True\n",
        "        },\n",
        "        \"compound\": {\n",
        "            \"exploit_count\": 1,       # 2021 $147M (tools: no new core, forks like Onyx/Sonne exploited)\n",
        "            \"recent_vuln\": 0,         # No recent core (tools: old audits)\n",
        "            \"audit_count\": 11,        # Mostly pre-2023 (OpenZeppelin/Trail of Bits)\n",
        "            \"latest_audit_year\": 2020,\n",
        "            \"issues\": [\"No new 2023-2026 core; 2021 exploit $147M mitigated; forks vulnerable\"],\n",
        "            \"has_audits\": True,\n",
        "            \"audit_note\": \"Pre-2023 (Trail of Bits, OpenZeppelin); no recent audits found, $1M bounty active\",\n",
        "            \"bounty_active\": True\n",
        "        }\n",
        "    }\n",
        "    data = history[protocol]\n",
        "    data.update(bounty_code_data)\n",
        "\n",
        "    base = 10\n",
        "    audit_bonus = min(data[\"audit_count\"] / 3, 4)     # Spread based on count\n",
        "    exploit_penalty = data[\"exploit_count\"] * 3\n",
        "    vuln_penalty = data[\"recent_vuln\"] * 1.5          # Higher for multiples\n",
        "    bounty_bonus = 0.5 if data['bounty_size'] > 1000000 else 0\n",
        "    code_penalty = -1 if data['code_age_days'] > 365 else 0\n",
        "    recency_penalty = 0\n",
        "    if data[\"latest_audit_year\"] < 2026:\n",
        "        recency_penalty = 0.5\n",
        "    if data[\"latest_audit_year\"] < 2025:\n",
        "        recency_penalty = 1.5\n",
        "    if data[\"latest_audit_year\"] < 2023:\n",
        "        recency_penalty = 3\n",
        "    security = base - exploit_penalty - vuln_penalty - recency_penalty + audit_bonus + bounty_bonus + code_penalty\n",
        "    security = max(0, min(10, security))\n",
        "\n",
        "    return {\n",
        "        \"exploit_count\": data[\"exploit_count\"],\n",
        "        \"issues\": data[\"issues\"],\n",
        "        \"has_audits\": data[\"has_audits\"],\n",
        "        \"audit_note\": data[\"audit_note\"],\n",
        "        \"security\": round(security, 2),\n",
        "        \"bounty_size\": data[\"bounty_size\"],\n",
        "        \"code_age_days\": data[\"code_age_days\"]\n",
        "    }\n",
        "\n",
        "\n",
        "def predict_tvl_trend(tvl_history):\n",
        "    if len(tvl_history) < 2:\n",
        "        return {\"direction\": \"Unknown\", \"slope\": 0}\n",
        "    x = np.arange(len(tvl_history)).reshape(-1, 1)\n",
        "    y = np.array(tvl_history)\n",
        "    model = LinearRegression().fit(x, y)\n",
        "    slope = model.coef_[0]\n",
        "    direction = \"Declining\" if slope < -1e6 else \"Stable/Increasing\"  # A % calc would be more precise but -1e6 keeps it simple and works for the 3 protocols here\n",
        "    return {\"direction\": direction, \"slope\": slope}\n",
        "\n",
        "\n",
        "def compute_safety_scores(protocol, market, tvl, sec_data, trend, util_chains):\n",
        "    config = PROTOCOLS[protocol]\n",
        "    if not market or not tvl:\n",
        "        return {\"financial\": 5, \"security\": 5, \"operational\": 5, \"total\": 5}\n",
        "\n",
        "    price_trend = (market.get(\"price_change_7d\", 0) + market.get(\"price_change_30d\", 0)) / 2\n",
        "    vol_penalty = market.get(\"volatility\", 0) * 2\n",
        "    tvl_size_score = min(tvl[\"total_tvl\"] / 1e9, 10)\n",
        "    trend_penalty = -abs(trend[\"slope\"]) / 5e8 if trend[\"slope\"] < 0 else 0\n",
        "    util_penalty = -1 if (config['type'] == 'lending' and util_chains['utilization'] > 80) or (config['type'] == 'dex' and util_chains['utilization'] < 10) else 0\n",
        "    chain_penalty = -0.5 if util_chains['chain_count'] < 3 else 0\n",
        "    financial = tvl_size_score + price_trend / 5 - vol_penalty + trend_penalty + util_penalty + chain_penalty\n",
        "    financial = round(max(-10, min(financial, 10)), 2)          # Allowing negative scores provides better differentiation for very high-risk scenarios\n",
        "\n",
        "    operational_score = compute_operational_risk(protocol)\n",
        "    security = sec_data[\"security\"]\n",
        "\n",
        "    total = round((financial + security + operational_score) / 3, 2)\n",
        "\n",
        "    return {\"financial\": financial, \"security\": security, \"operational\": operational_score, \"total\": total}\n",
        "\n",
        "\n",
        "def compute_operational_risk(protocol):\n",
        "    # The notebook aims to be lightweight and avoid heavy external dependencies, so some data like this is pre-filled or simulated.\n",
        "    try:\n",
        "        if protocol == \"aave\":\n",
        "            active_users = 150000\n",
        "            oracle_risk = 1\n",
        "        elif protocol == \"uniswap\":\n",
        "            active_users = 200000\n",
        "            oracle_risk = 1\n",
        "        else:\n",
        "            active_users = 50000\n",
        "            oracle_risk = 1\n",
        "        user_penalty = -1 if active_users < 100000 else 0\n",
        "        op = 10 + user_penalty - oracle_risk\n",
        "        op = max(0, min(10, op))\n",
        "        return round(op, 2)\n",
        "    except:\n",
        "        return 8.0"
      ],
      "metadata": {
        "id": "Se9F1EDFHxru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 4 – Visualization & PDF generation"
      ],
      "metadata": {
        "id": "MDs9qWnqgzmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_tvl_forecast(protocol, tvl_history):\n",
        "    if len(tvl_history) < 2:\n",
        "        return None\n",
        "    x_historical_days = np.arange(-len(tvl_history) + 1, 1)\n",
        "    y_historical_tvl_values = np.array(tvl_history)\n",
        "    model = LinearRegression().fit(x_historical_days.reshape(-1,1), y_historical_tvl_values)\n",
        "    x_future_days = np.arange(1, 8)\n",
        "    y_forecasted_tvl_values = model.predict(x_future_days.reshape(-1,1))\n",
        "\n",
        "    fig = plt.figure(figsize=(9,4.5))\n",
        "    plt.plot(x_historical_days, y_historical_tvl_values / 1e9, 'o-', label=\"Historical\", color=\"blue\")\n",
        "    plt.plot(x_future_days, y_forecasted_tvl_values / 1e9, '--', label=\"7-day forecast\", color=\"red\")\n",
        "    plt.title(f\"{protocol.upper()} TVL Trend & Forecast ($B)\")\n",
        "    plt.xlabel(\"Days (0 = most recent)\")\n",
        "    plt.ylabel(\"TVL ($B)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "def export_pdf_report(results):\n",
        "    pdf = FPDF()\n",
        "    pdf.set_auto_page_break(auto=True, margin=15)\n",
        "\n",
        "    # Page 1: Per-protocol details\n",
        "    pdf.add_page()\n",
        "    pdf.set_font(\"Helvetica\", \"B\", 14)\n",
        "    pdf.cell(0, 10, \"DeFi Protocol Safety Report (Feb 2026)\", new_x=XPos.LMARGIN, new_y=YPos.NEXT, align=\"C\")\n",
        "    pdf.set_font(\"Helvetica\", \"I\", 8)\n",
        "    pdf.multi_cell(0, 5, \"Disclaimer: Basic risk tool using public data. Not financial advice or full audit.\")\n",
        "    pdf.ln(5)\n",
        "\n",
        "    for proto, res in results.items():\n",
        "        pdf.set_font(\"Helvetica\", \"B\", 12)\n",
        "        pdf.cell(0, 5, f\"{proto.upper():<10} {res['total']:.2f}/10 -> {res['rating']}\", new_x=XPos.LMARGIN, new_y=YPos.NEXT)\n",
        "        pdf.set_font(\"Helvetica\", size=10)\n",
        "        pdf.cell(0, 5, f\"Financial : {res['financial']:>6.2f}/10 | Security: {res['security']:>6.2f}/10\", new_x=XPos.LMARGIN, new_y=YPos.NEXT)\n",
        "        pdf.cell(0, 5, f\"TVL       : $ {res['tvl']['total_tvl']/1e9:>5.1f}B | Trend: {res['trend']['direction']}\", new_x=XPos.LMARGIN, new_y=YPos.NEXT)\n",
        "        pdf.cell(0, 5, f\"{res['util_chains']['metric_name']}: {res['util_chains']['utilization']:.1f}% | Chains: {res['util_chains']['chain_count']}\", new_x=XPos.LMARGIN, new_y=YPos.NEXT)\n",
        "        pdf.cell(0, 5, f\"Data Source: {res['data_source']}\", new_x=XPos.LMARGIN, new_y=YPos.NEXT)\n",
        "        pdf.cell(0, 5, f\"Audited   : {'Yes' if res['sec']['has_audits'] else 'No'} | Exploits: {res['sec']['exploit_count']}\", new_x=XPos.LMARGIN, new_y=YPos.NEXT)\n",
        "        audit_note = res['sec']['audit_note']\n",
        "        bounty_size = res['sec']['bounty_size']\n",
        "        code_age_days = res['sec']['code_age_days']\n",
        "        if bounty_size >= 1_000_000:\n",
        "            bounty_display = f\"${bounty_size / 1_000_000:.1f}M\"\n",
        "        else:\n",
        "            bounty_display = f\"${bounty_size:,}\"\n",
        "        pdf.multi_cell(0, 5, f\"Audit Note: {audit_note} | Bounty: {bounty_display} | Code Age: {code_age_days} days\", new_x=XPos.LMARGIN, new_y=YPos.NEXT)\n",
        "        issues_str = '; '.join(res['sec']['issues']) or \"None major\"\n",
        "        pdf.multi_cell(0, 5, f\"Issues    : {issues_str}\", new_x=XPos.LMARGIN, new_y=YPos.NEXT)\n",
        "        pdf.ln(3)\n",
        "\n",
        "    # Summary table\n",
        "    pdf.ln(15)\n",
        "    pdf.set_font(\"Helvetica\", \"B\", 14)\n",
        "    pdf.cell(0, 10, \"Overall Protocol Summary\", new_x=XPos.LMARGIN, new_y=YPos.NEXT, align=\"C\")\n",
        "    pdf.ln(5)\n",
        "    col_widths = [40, 20, 20, 20, 20, 30, 20, 20]\n",
        "    pdf.set_font(\"Helvetica\", \"B\", 10)\n",
        "    headers = ['Protocol', 'Total Score', 'Financial', 'Security', 'TVL(B)', 'Trend', 'Audit', 'Exploits']\n",
        "    for i, header in enumerate(headers):\n",
        "        pdf.cell(col_widths[i], 7, header, border=1, align='C')   # centering headers with borders\n",
        "    pdf.ln()\n",
        "    pdf.set_font(\"Helvetica\", size=9)\n",
        "    aligns = ['L', 'R', 'R', 'R', 'R', 'L', 'C', 'R']             # Alignments in table cells\n",
        "    for proto, res in results.items():\n",
        "        row_data = [\n",
        "            proto.upper(),\n",
        "            f\"{res['total']:.2f}/10.00\",\n",
        "            f\"{res['financial']:.2f}\",\n",
        "            f\"{res['security']:.2f}\",\n",
        "            f\"${res['tvl']['total_tvl']/1e9:.1f}\",\n",
        "            res['trend']['direction'],\n",
        "            'Yes' if res['sec']['has_audits'] else 'No',\n",
        "            str(res['sec']['exploit_count'])\n",
        "        ]\n",
        "        for i, item in enumerate(row_data):\n",
        "            pdf.cell(col_widths[i], 6, item, border=1, align=aligns[i])\n",
        "        pdf.ln()\n",
        "\n",
        "\n",
        "    # Page 2: Heatmap (center=0 to handle negatives better)\n",
        "    pdf.add_page()\n",
        "    pdf.set_font(\"Helvetica\", \"B\", 14)\n",
        "    pdf.cell(0, 10, \"Safety Heatmap (Higher = Safer)\", new_x=XPos.LMARGIN, new_y=YPos.NEXT, align=\"C\")\n",
        "    pdf.ln(5)\n",
        "\n",
        "    data = pd.DataFrame({\n",
        "        \"Protocol\": [p.upper() for p in results],\n",
        "        \"Financial\": [results[p][\"financial\"] for p in results],\n",
        "        \"Security\": [results[p][\"security\"] for p in results],\n",
        "        \"Total\": [results[p][\"total\"] for p in results]\n",
        "    }).set_index(\"Protocol\")\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8,4))\n",
        "    sns.heatmap(data.T, annot=True, cmap=\"RdYlGn\", center=5, fmt=\".1f\", cbar_kws={'label': 'Score'})  # Center=5, yellow for better representation of risk on a 0-10 scale\n",
        "    plt.title(\"Component Scores\")\n",
        "    buf = BytesIO()\n",
        "    plt.savefig(buf, format=\"png\", dpi=200, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "    buf.seek(0)\n",
        "    pdf.image(buf, x=15, w=180)\n",
        "\n",
        "    y_after_heatmap_image = pdf.get_y()     # Store current Y position after heatmap\n",
        "\n",
        "    # Footer if a fallback is used\n",
        "    if any('Fallback' in res['data_source'] for res in results.values()):\n",
        "        pdf.set_font(\"Helvetica\", \"I\", 8)\n",
        "        footer_height = 10\n",
        "        y_bottom_of_page = pdf.h - pdf.b_margin - footer_height\n",
        "        pdf.set_y(y_bottom_of_page)\n",
        "        pdf.cell(0, footer_height, \"Note: Fallback data from Feb 2026 cache used for some metrics.\", align=\"C\")\n",
        "        pdf.set_y(y_after_heatmap_image)\n",
        "\n",
        "\n",
        "    # Pages 3–5: TVL Charts\n",
        "    for proto, res in results.items():\n",
        "        pdf.add_page()\n",
        "        pdf.set_font(\"Helvetica\", \"B\", 14)\n",
        "        pdf.cell(0, 10, f\"TVL Trend - {proto.upper()}\", new_x=XPos.LMARGIN, new_y=YPos.NEXT, align=\"C\")\n",
        "        pdf.ln(5)\n",
        "        fig = plot_tvl_forecast(proto, res[\"tvl\"][\"tvl_history\"])\n",
        "        if fig:\n",
        "            buf = BytesIO()\n",
        "            fig.savefig(buf, format=\"png\", dpi=200, bbox_inches=\"tight\")\n",
        "            plt.close(fig)\n",
        "            buf.seek(0)\n",
        "            pdf.image(buf, x=15, w=180)\n",
        "        else:\n",
        "            pdf.cell(0, 10, \"Not enough data for chart\", align=\"C\")\n",
        "\n",
        "    filename = \"DeFi_Safety_Report.pdf\"\n",
        "    pdf.output(filename)\n",
        "    print(f\"PDF saved: {filename}\")\n",
        "    files.download(filename)\n",
        "    return filename"
      ],
      "metadata": {
        "id": "rFuQ5eeF6peq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 5 – Run everything cell loop"
      ],
      "metadata": {
        "id": "FK7Tp3UliF3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "\n",
        "for proto in PROTOCOLS:\n",
        "    print(f\"\\nAnalyzing {proto.upper()}\")\n",
        "    market = fetch_market_data(proto)\n",
        "    tvl = fetch_tvl_data(proto)\n",
        "    util_chains = fetch_utilization_and_chains(proto, tvl)\n",
        "    bounty_code = fetch_bounty_and_code_age(proto)\n",
        "    sec = check_security_history(proto, bounty_code)\n",
        "    trend = predict_tvl_trend(tvl[\"tvl_history\"])\n",
        "    scores = compute_safety_scores(proto, market, tvl, sec, trend, util_chains)\n",
        "\n",
        "    if scores[\"total\"] > 7:\n",
        "        rating = \"LOW RISK\"\n",
        "    elif scores[\"total\"] > 4:\n",
        "        rating = \"MEDIUM RISK\"\n",
        "    else:\n",
        "        rating = \"HIGH RISK\"\n",
        "\n",
        "    sources = [market.get('source', 'unknown'), tvl.get('source', 'unknown'), util_chains.get('source', 'unknown')]\n",
        "    data_source = 'Live API' if all(s == 'live' for s in sources) else 'Fallback (cached data used)'\n",
        "\n",
        "    results[proto] = {\n",
        "        \"total\": scores[\"total\"],\n",
        "        \"financial\": scores[\"financial\"],\n",
        "        \"security\": scores[\"security\"],\n",
        "        \"operational\": scores[\"operational\"],\n",
        "        \"rating\": rating,\n",
        "        \"tvl\": tvl,\n",
        "        \"trend\": trend,\n",
        "        \"sec\": sec,\n",
        "        \"data_source\": data_source,\n",
        "        \"util_chains\": util_chains\n",
        "    }\n",
        "\n",
        "export_pdf_report(results)"
      ],
      "metadata": {
        "id": "oEKPdOTAkWsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Past Results & Analysis\n",
        "\n",
        "Running the tool produces a new PDF, resembling the screenshots in the screenshots folder in the repo, which are also called below.\n",
        "Actual numbers shift with market data and also depend on live API data vs fallback."
      ],
      "metadata": {
        "id": "1fW8pzWgoJK_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example-Run Results Showcase"
      ],
      "metadata": {
        "id": "SHN_RZKztv02"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d064b4f6"
      },
      "source": [
        "# Important!!!\n",
        "\n",
        "# The following results are not the most recent run output of the code. This cell showcases the uploaded screenshots from the screenshots folder in the repo, produced on a previous date.\n",
        "# The output from your most recent run of the code is the PDF file downloaded on your system.\n",
        "\n",
        "\n",
        "from IPython.display import Image\n",
        "\n",
        "# Display the report summary page 1\n",
        "github_image_url_1 = 'https://raw.githubusercontent.com/GeorgeKGM2058/DeFi-Protocol-Safety-Scoring-Tool/refs/heads/main/screenshots/report-summary-page1.png'\n",
        "print(\"Report Summary - Page 1:\")\n",
        "display(Image(url=github_image_url_1))\n",
        "\n",
        "# Display the report heatmap page 2\n",
        "github_image_url_2 = 'https://raw.githubusercontent.com/GeorgeKGM2058/DeFi-Protocol-Safety-Scoring-Tool/refs/heads/main/screenshots/report-heatmap-page2.png'\n",
        "print(\"\\nReport Heatmap - Page 2:\")\n",
        "display(Image(url=github_image_url_2))\n",
        "\n",
        "# Display the report TVL Aave page 3\n",
        "github_image_url_3 = 'https://raw.githubusercontent.com/GeorgeKGM2058/DeFi-Protocol-Safety-Scoring-Tool/refs/heads/main/screenshots/report-tvl-aave-page3.png'\n",
        "print(\"\\nReport TVL Aave - Page 3:\")\n",
        "display(Image(url=github_image_url_3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From that specific run:\n",
        "\n",
        "- Aave often leads with mid-6s total, solid TVL cushions financial hits and fresh 2026 audits keep security near-perfect.\n",
        "- Uniswap hovers around high-2s. Smaller TVL means more exposure to volatility, but strong bounty program and 2024 audits help security.\n",
        "- Compound trails in low-2s: The old 2021 hack and dated audits drag security down, plus bear trends hit financial hard.\n",
        "\n",
        "The heatmap makes differences pop. Greens for Aave's strengths, reds for others' financial weaknesses. TVL charts show steady declines across the board in this 2026 bear phase, with forecasts extending the line for a quick \"what if\" view.\n",
        "Overall, it highlights how even top protocols vary. Scale protects Aave, but smaller ones like Compound feel every market dip more.\n",
        "Limitations: Historical focus means it misses emerging threats and linear trends oversimplify volatility."
      ],
      "metadata": {
        "id": "DjoQhVhwoTqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Conclusion\n",
        "\n",
        "Wrapping up, this tool strips DeFi risk assessment down to essentials: Grab data, score simply, report clearly. It's lightweight enough for quick checks, but expandable.\n",
        "Building it showed me how a few APIs and basic Python can turn raw numbers into useful insights."
      ],
      "metadata": {
        "id": "IcbVPK1Joc8o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "References\n",
        "\n",
        "\n",
        "CoinGecko API docs (2026) – https://www.coingecko.com/en/api\n",
        "\n",
        "DefiLlama protocol data – https://defillama.com/docs/api\n",
        "\n",
        "Rekt.news exploit database – https://rekt.news\n",
        "\n",
        "Immunefi bounty listings – https://immunefi.com\n",
        "\n",
        "Aave developer docs & audits – https://docs.aave.com\n",
        "\n",
        "Uniswap governance & security – https://uniswap.org\n",
        "\n",
        "Compound finance resources – https://compound.finance/docs"
      ],
      "metadata": {
        "id": "ds7lKN_SogTS"
      }
    }
  ]
}